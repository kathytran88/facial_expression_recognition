{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_M90-_f3k8m"
      },
      "source": [
        "# Dataset available at kaggle\n",
        "\n",
        "https://www.kaggle.com/jonathanoheix/face-expression-recognition-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVZJB6C5vobN"
      },
      "source": [
        "# Install libraries, packages and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPaMVYICuTRa",
        "outputId": "8f584929-7e8b-47ef-e844-5124210f2113"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/parth1620/Facial-Expression-Dataset.git\n",
        "#!pip install -U git+https://github.com/albumentations-team/albumentations\n",
        "#!pip install timm\n",
        "#!pip install --upgrade opencv-contrib-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoVVdBpoweEk"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "creZrxH0vv20"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5604GKdwtAr"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dd6v8vyGwp3F"
      },
      "outputs": [],
      "source": [
        "TRAIN_IMG_FOLDER_PATH = '/Users/trananhngan/Downloads/ML_model/Facial-Expression-Dataset/train/'\n",
        "VALID_IMG_FOLDER_PATH = '/Users/trananhngan/Downloads/ML_model/Facial-Expression-Dataset/validation/'\n",
        "\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 32 \n",
        "EPOCHS = 100 \n",
        "DEVICE = 'cpu'\n",
        "MODEL_NAME = 'efficientnet_b0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn5jWAF9xDUY"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uvujfkVHxJ0V"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jqf-Ktqsx9WL"
      },
      "outputs": [],
      "source": [
        "# Data augmentation\n",
        "train_augs = T.Compose([\n",
        "    T.RandomHorizontalFlip(p = 0.5),\n",
        "    T.RandomRotation(degrees=(-20, +20)),\n",
        "    T.ToTensor() #PIL / numpy arr -> torch tensor -> (h,w,c) -> (c,h,w)\n",
        "])\n",
        "valid_augs = T.Compose([ # converting the image into a Tensor array, which can be changed to different dimensions.\n",
        "    T.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oeU1NfmYxC7J"
      },
      "outputs": [],
      "source": [
        "trainset = ImageFolder(TRAIN_IMG_FOLDER_PATH, transform=train_augs)\n",
        "validset = ImageFolder(VALID_IMG_FOLDER_PATH, transform=valid_augs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eznRIKAzzuCj",
        "outputId": "2e2b3bb3-f07d-485e-eac8-b14624a4e480"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total no. of examples in trainset : 57528\n",
            "Total no. of examples in validset : 14244\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total no. of examples in trainset : {len(trainset)}\")\n",
        "print(f\"Total no. of examples in validset : {len(validset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtoA4SRny_vz",
        "outputId": "35f52f84-be78-44a1-fb17-c2e8ef8c4dc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'angry': 0, 'confident': 1, 'disgust': 2, 'fear': 3, 'happy': 4, 'neutral': 5, 'sad': 6, 'surprise': 7, 'unconfident': 8}\n"
          ]
        }
      ],
      "source": [
        "print(trainset.class_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n"
          ]
        }
      ],
      "source": [
        "num_of_class = len(trainset.class_to_idx)\n",
        "print(num_of_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "0-b1SLqNw91J",
        "outputId": "a3a585bb-9feb-4fee-97b9-59250b869ed8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "image, label = trainset[20]\n",
        "plt.imshow(image.permute(1,2,0))\n",
        "plt.title(label);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjVmykAGy0Sh"
      },
      "source": [
        "# Load Dataset into Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "egpuikhHymP7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Pea8G9-HzhuZ"
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\n",
        "validloader = DataLoader(validset, batch_size = BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjf9oeOBzihh",
        "outputId": "46f50fc6-50c8-4088-dae6-a340f232352b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total no. of batches in trainloader : 1798\n",
            "Total no. of batches in validloader : 446\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total no. of batches in trainloader : {len(trainloader)}\")\n",
        "print(f\"Total no. of batches in validloader : {len(validloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtVSVVj-zp1C",
        "outputId": "13c4a9e3-e1f7-4ba7-b751-d596ffe46ff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One image batch shape : torch.Size([32, 3, 48, 48])\n",
            "One label batch shape : torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "for images, labels in trainloader:\n",
        "  break;\n",
        "\n",
        "print(f\"One image batch shape : {images.shape}\")\n",
        "print(f\"One label batch shape : {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQvgvbKu0TUG"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.0.7)\n",
            "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from timm) (2.2.2)\n",
            "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from timm) (0.17.2)\n",
            "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from timm) (0.23.4)\n",
            "Requirement already satisfied: safetensors in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface_hub->timm) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/trananhngan/Library/Python/3.12/lib/python/site-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch->timm) (1.12.1)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision->timm) (10.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch->timm) (1.3.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iqwWf5-O0SWP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import timm\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "U6uJZmUw0XJX"
      },
      "outputs": [],
      "source": [
        "class FaceModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FaceModel, self).__init__()\n",
        "    self.eff_net = timm.create_model('efficientnet_b0', pretrained = True, num_classes = 9)\n",
        "\n",
        "  def forward(self, images, labels = None):\n",
        "    logits = self.eff_net(images)\n",
        "\n",
        "    if labels != None:\n",
        "      loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "      return logits, loss\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "2515ce85521c413491f8cc7bddd4210f",
            "676ebeab1bbb445c8c949d6cacce4ed1",
            "ded5e8007f9d4aa9a6adc11ef906494e",
            "f60d3dc094654f4396771898d968d5d7",
            "3be7cffa607f423bbfd75ac05822d6aa",
            "d1eee20181294c3d86fc690c03b76a67",
            "e27429f277894f81a7243209ca1be962",
            "60f266f9cbd84ca4b39c14b9db2fa2ed",
            "83a9173c16f84212817044bbb5cf0df3",
            "e14fca970f76431da5a445553b367867",
            "3f302acf97ba48a68f4cb33dcc42a1b8"
          ]
        },
        "id": "hgp5OjEO0_2w",
        "outputId": "b68666f8-0eb3-4432-af8a-a17043c6c97a"
      },
      "outputs": [],
      "source": [
        "model = FaceModel()\n",
        "model.to(DEVICE);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzDb6TdaCpF5"
      },
      "source": [
        "# Create Train and Eval Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eLQVQA0PVjX8"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "esp-tSrxTiPX"
      },
      "outputs": [],
      "source": [
        "def multiclass_accuracy(y_pred,y_true):\n",
        "    top_p,top_class = y_pred.topk(1,dim = 1)\n",
        "    equals = top_class == y_true.view(*top_class.shape)\n",
        "    return torch.mean(equals.type(torch.FloatTensor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QBim_pZxColX"
      },
      "outputs": [],
      "source": [
        "def train_fn(model, dataloader, optimizer, current_epo):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  total_acc = 0.0\n",
        "  tk = tqdm(dataloader, desc = \"EPOCH\" + \"[TRAIN]\" + str(current_epo + 1) + '/' + str(EPOCHS))\n",
        "\n",
        "  for t,data in enumerate(tk):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model(images, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    total_acc += multiclass_accuracy(logits, labels)\n",
        "    tk.set_postfix({'loss': '%6f' %float(total_loss / (t+1)), 'acc': '%6f' %float(total_acc / (t+1)),})\n",
        "\n",
        "  return total_loss / len(dataloader), total_acc / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "T3eGYNqR1FeL"
      },
      "outputs": [],
      "source": [
        "def eval_fn(model, dataloader, current_epo):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "  total_acc = 0.0\n",
        "  tk = tqdm(dataloader, desc = \"EPOCH\" + \"[VALID]\" + str(current_epo + 1) + '/' + str(EPOCHS))\n",
        "\n",
        "  for t,data in enumerate(tk):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "    logits, loss = model(images, labels)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    total_acc += multiclass_accuracy(logits, labels)\n",
        "    tk.set_postfix({'loss': '%6f' %float(total_loss / (t+1)), 'acc': '%6f' %float(total_acc / (t+1)),})\n",
        "\n",
        "  return total_loss / len(dataloader), total_acc / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2YsWQvsXiKG"
      },
      "source": [
        "# Create Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mbKd9D7pWeM_"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8yzdLmLXkC-",
        "outputId": "e930b87d-9027-43f9-e2ff-21a3b40679c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EPOCH[TRAIN]1/100:   0%|          | 0/1798 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "Target 8 is out of bounds.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m best_valid_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mInf\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 4\u001b[0m   train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m   valid_loss, valid_acc \u001b[38;5;241m=\u001b[39m eval_fn(model, validloader, i)\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m valid_loss \u001b[38;5;241m<\u001b[39m best_valid_loss:\n",
            "Cell \u001b[0;32mIn[20], line 12\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(model, dataloader, optimizer, current_epo)\u001b[0m\n\u001b[1;32m      9\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[16], line 10\u001b[0m, in \u001b[0;36mFaceModel.forward\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m      7\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meff_net(images)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 8 is out of bounds."
          ]
        }
      ],
      "source": [
        "best_valid_loss = np.Inf\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "  train_loss, train_acc = train_fn(model, trainloader, optimizer, i)\n",
        "  valid_loss, valid_acc = eval_fn(model, validloader, i)\n",
        "\n",
        "  if valid_loss < best_valid_loss:\n",
        "    torch.save(model.state_dict(), 'best-weights.pt')\n",
        "    print('SAVE-BEST-WEIGHTS')\n",
        "    best_valid_loss = valid_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjiklrErXkZ6"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iozgi4xhLNg"
      },
      "outputs": [],
      "source": [
        "classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "def view_classify(img, ps):\n",
        "\n",
        "    ps = ps.data.cpu().numpy().squeeze()\n",
        "    img = img.numpy().transpose(1,2,0)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(5,9), ncols=2)\n",
        "    ax1.imshow(img)\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(classes, ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(classes)\n",
        "    ax2.set_yticklabels(classes)\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "nPryzS0-b6AW",
        "outputId": "8b78f043-52b0-49d9-fda4-ba40c44370f9"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'torch.cuda' has no attribute 'clear_cache'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-1ae8e65dba94>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'clear_cache'"
          ]
        }
      ],
      "source": [
        "#torch.cuda.clear_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "y_QZvnUGXmOf",
        "outputId": "855c73a5-48db-43a3-ba63-383a76114216"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Users/trananhngan/Downloads/ML_model/img2.jpg'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load and transform the image\u001b[39;00m\n\u001b[1;32m      8\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg2.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Define the transformation to convert the image to tensor and normalize it\u001b[39;00m\n\u001b[1;32m     12\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     13\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     14\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     15\u001b[0m ])\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/PIL/Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/trananhngan/Downloads/ML_model/img2.jpg'"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "label = 0\n",
        "\n",
        "# Load and transform the image\n",
        "image_path = 'img2.jpg'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Define the transformation to convert the image to tensor and normalize it\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "image = transform(image)\n",
        "image = image.unsqueeze(0)\n",
        "\n",
        "logits = model(image.to(DEVICE))\n",
        "probs = nn.Softmax(dim=1)(logits)\n",
        "\n",
        "view_classify(image.squeeze(), probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqeEhuSigBWn"
      },
      "source": [
        "### Webcam Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.8 MB 18.1 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.3 in /Users/joshuaalfred/Library/Python/3.9/lib/python/site-packages (from opencv-python) (1.26.4)\n",
            "Installing collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.10.0.84\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.1.2 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install opencv-python\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiCTz-RAf0Nm"
      },
      "outputs": [],
      "source": [
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8hpEgXCgEmA"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "model = FaceModel()\n",
        "model.load_state_dict(torch.load('/Users/trananhngan/Downloads/ML_model/best-weights.pt', map_location=torch.device('cpu')))\n",
        "\n",
        "device = torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "classes ={0: 'angry', 1:'disgust', 2:'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'suprise'}\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    face = transform(rgb_frame)\n",
        "    face = face.unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(face.to(device))\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        emotion = classes[int(predicted)]\n",
        "\n",
        "\n",
        "    cv2.putText(frame, emotion, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "    cv2.imshow('Webcam', frame)\n",
        "\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2515ce85521c413491f8cc7bddd4210f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_676ebeab1bbb445c8c949d6cacce4ed1",
              "IPY_MODEL_ded5e8007f9d4aa9a6adc11ef906494e",
              "IPY_MODEL_f60d3dc094654f4396771898d968d5d7"
            ],
            "layout": "IPY_MODEL_3be7cffa607f423bbfd75ac05822d6aa"
          }
        },
        "3be7cffa607f423bbfd75ac05822d6aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f302acf97ba48a68f4cb33dcc42a1b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60f266f9cbd84ca4b39c14b9db2fa2ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "676ebeab1bbb445c8c949d6cacce4ed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1eee20181294c3d86fc690c03b76a67",
            "placeholder": "​",
            "style": "IPY_MODEL_e27429f277894f81a7243209ca1be962",
            "value": "model.safetensors: 100%"
          }
        },
        "83a9173c16f84212817044bbb5cf0df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1eee20181294c3d86fc690c03b76a67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ded5e8007f9d4aa9a6adc11ef906494e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60f266f9cbd84ca4b39c14b9db2fa2ed",
            "max": 21355344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83a9173c16f84212817044bbb5cf0df3",
            "value": 21355344
          }
        },
        "e14fca970f76431da5a445553b367867": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e27429f277894f81a7243209ca1be962": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f60d3dc094654f4396771898d968d5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e14fca970f76431da5a445553b367867",
            "placeholder": "​",
            "style": "IPY_MODEL_3f302acf97ba48a68f4cb33dcc42a1b8",
            "value": " 21.4M/21.4M [00:00&lt;00:00, 78.8MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
